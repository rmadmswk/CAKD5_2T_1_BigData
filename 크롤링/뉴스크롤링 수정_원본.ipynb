{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e096ab4f",
   "metadata": {},
   "source": [
    "#### 소스 출처: https://github.com/woonhy/newscrawler/blob/master/Article_Parse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e32f87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: euc-kr -*-\n",
    "# 사용할 라이브러리\n",
    "import calendar\n",
    "import requests\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\" }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d6810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스내용전처리(특수문자제거)\n",
    "def Clearcontent(text):\n",
    "    first = re.sub('[\\{\\}\\[\\]\\/?,;:|\\)*~`!^\\-_+<>@\\#$%&n▲▶◆◀■\\\\\\=\\(\\'\\\"]','', text)\n",
    "    second = re.sub('본문 내용|TV플레이어| 동영상 뉴스|flash 오류를 우회하기 위한 함수 추가fuctio flashremoveCallback|tt|t|앵커 멘트|xa0', '', first)\n",
    "    Third = second.strip().replace('   ', '')  # 공백 에러 삭제\n",
    "    Four = ''.join(reversed(Third))  # 기사 내용을 reverse 한다.\n",
    "    content = ''\n",
    "    for i in range(0, len(Third)):\n",
    "        if Four[i:i+2] == '.다':  # reverse 된 기사 내용중, \".다\"로 끝나는 경우 기사 내용이 끝난 것이기 때문에 기사 내용이 끝난 후의 광고, 기자 등의 정보는 다 지운다.\n",
    "            content = ''.join(reversed(Four[i:]))\n",
    "            break\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bed924c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헤드라인 문자 정리\n",
    "def Clearheadline(text):\n",
    "    first = re.sub('[\\{\\}\\[\\]\\/?,;:|\\)*~`!^\\-_+<>@\\#$%&n▲▶◆◀■\\\\\\=\\(\\'\\\"]', '', text)\n",
    "    return first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b92468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공부\n",
    "def html_totalpage(url):\n",
    "    totlapage_url = url       #마지막 url\n",
    "    request_content = requests.get(totlapage_url,headers=headers)\n",
    "    document_content = BeautifulSoup(request_content.content, 'html.parser')\n",
    "    Tag_headline = document_content.find('div', {'class': 'paging'}).find('strong')\n",
    "    regex = re.compile(r'<strong>(?P<num>\\d+)')\n",
    "    match = regex.findall(str(Tag_headline))\n",
    "    return int(match[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bc807a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색할 url생성\n",
    "def Make_url(URL, startyear, lastyear, startmonth, lastmonth):\n",
    "    Maked_url = []    #검색url리스트 생성\n",
    "    final_startmonth = startmonth\n",
    "    final_lastmonth = lastmonth\n",
    "    for year in range(startyear, lastyear + 1):   #url날짜설정\n",
    "        if year != lastyear:\n",
    "            startmonth = 1\n",
    "            lastmonth = 12\n",
    "        else:\n",
    "            startmonth = final_startmonth\n",
    "            lastmonth = final_lastmonth\n",
    "            \n",
    "        for Month in range(startmonth, lastmonth + 1):\n",
    "            for Month_Day in range(1, calendar.monthrange(year, Month)[1] + 1):\n",
    "                url = URL\n",
    "                if len(str(Month)) == 1:\n",
    "                    Month = \"0\" + str(Month)\n",
    "                if len(str(Month_Day)) == 1:\n",
    "                    Month_Day = \"0\" + str(Month_Day)\n",
    "                url = url + str(year) + str(Month) + str(Month_Day)\n",
    "                final_url = url  # page 날짜 정보만 있고 page 정보가 없는 url 임시 저장\n",
    "                totalpage = html_totalpage(final_url+\"&page=1000\") # totalpage는 네이버 페이지 구조를 이용해서 page=1000으로 지정해 totalpage를 알아냄 ( page=1000을 입력할 경우 페이지가 존재하지 않기 때문에 page=totalpage로 이동 됨)\n",
    "                for page in range(1, totalpage + 1):\n",
    "                    url = final_url # url page 초기화\n",
    "                    url = url + \"&page=\" + str(page)\n",
    "                    Maked_url.append(url)\n",
    "    return Maked_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "513a92cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_url(url):\n",
    "    new_url = url[::-1]\n",
    "    idx=[]\n",
    "    for i,v in enumerate(new_url):\n",
    "        if v.split('&')[4] == 'page=1':\n",
    "            idx.append(i)\n",
    "        else :\n",
    "            break\n",
    "    new_url=np.delete(new_url, idx)\n",
    "    return new_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a5eef6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8016/3372114679.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mcategory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0murl_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# URL 인덱스와 Category 인덱스가 일치할 경우 그 값도 일치\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Article_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcategory\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'euc-kr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mwcsv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"http://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=\"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"&date=\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "# url_list = [100, 101,102, 103, 104, 105]\n",
    "# Category = [\"정치\", \"경제\" \"사회\", \"생활문화\", \"세계\", \"IT과학\"]\n",
    "url_list = [100]\n",
    "Category = [\"정치\"]\n",
    "start =0\n",
    "end_size = 100\n",
    "\n",
    "\n",
    "for url_num in url_list:  # URL 카테고리\n",
    "    try:\n",
    "        category = Category[url_list.index(url_num)]  # URL 인덱스와 Category 인덱스가 일치할 경우 그 값도 일치\n",
    "        file = open('Article_' + category + '.csv', 'w', encoding='euc-kr', newline='')\n",
    "        wcsv = csv.writer(file)\n",
    "\n",
    "        url = \"http://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=\" +str(url_num)+\"&date=\"\n",
    "        final_urlday = Make_url(url, 2022, 2022, 2, 3) # 2017년 1월 ~ 2018년 6월 마지막 날까지 기사를 수집합니다.\n",
    "        final_urlday=re_url(final_urlday)\n",
    "        print(\"url success\")\n",
    "\n",
    "        for URL in final_urlday:\n",
    "            request = requests.get(URL, headers=headers)\n",
    "            document = BeautifulSoup(request.content, 'html.parser')\n",
    "            Tag = document.find_all('dt', {'class': 'photo'})\n",
    "\n",
    "            post = []\n",
    "            for tag in Tag:\n",
    "                post.append(tag.a.get('href'))  # 해당되는 page에서 모든 기사들의 URL을 post 리스트에 넣음\n",
    "\n",
    "            for content_url in post:  # 기사 URL\n",
    "                if (start%100)==0:\n",
    "                    print(start)\n",
    "                start +=1\n",
    "                sleep(0.01)\n",
    "                request_content = requests.get(content_url, headers=headers)\n",
    "                document_content = BeautifulSoup(request_content.content, 'html.parser')\n",
    "\n",
    "                try:\n",
    "                    Tag_headline = document_content.find_all('h3', {'id': 'articleTitle'}, {'class': 'tts_head'})\n",
    "                    text_headline = ''  # 뉴스 기사 제목 초기화\n",
    "                    text_headline = text_headline + Clearheadline(str(Tag_headline[0].find_all(text=True)))\n",
    "                    if not text_headline:  # 공백일 경우 기사 제외 처리\n",
    "                        continue\n",
    "\n",
    "                    Tag_content = document_content.find_all('div', {'id': 'articleBodyContents'})\n",
    "                    text_sentence = ''  # 뉴스 기사 본문 초기화\n",
    "                    text_sentence = text_sentence + Clearcontent(str(Tag_content[0].find_all(text=True)))\n",
    "                    if not text_headline: # 공백일 경우 기사 제외 처리\n",
    "                        continue\n",
    "\n",
    "                    Tag_company = document_content.find_all('meta', {'property': 'me2:category1'})\n",
    "                    text_company = ''  # 언론사 초기화\n",
    "                    text_company = text_company + str(Tag_company[0].get('content'))\n",
    "                    if not text_headline: # 공백일 경우 기사 제외 처리\n",
    "                        continue\n",
    "\n",
    "                    wcsv.writerow([text_headline, text_sentence, text_company, category])\n",
    "\n",
    "                except UnicodeEncodeError:  # UnicodeEncodeError ..\n",
    "                    pass\n",
    "                \n",
    "    except KeyboardInterrupt:     # 종료하고싶으면 중지\n",
    "                    file.close()\n",
    "                    break            \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d0efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
